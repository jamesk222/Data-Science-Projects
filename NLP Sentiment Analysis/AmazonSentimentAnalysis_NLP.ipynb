{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e7c4af-0c0f-4d53-b9a2-d35dc7ab2f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\kpk\n",
      "[nltk_data]     laptops\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "# Define stop words BEFORE using the function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Import the dataset\n",
    "df = pd.read_csv(\"D:/DataScience/Nlp/Reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d19f29-ccdc-4ef3-ad74-27f3be715600",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba6e698-12ab-4d66-8023-cec6067d55ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a71b5e9-9018-4448-ba49-81c1c7ea1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only relevant columns\n",
    "df = df[['Score','Text']].dropna()\n",
    "# Map numerical rating to sentiment\n",
    "def map_sentiment(Score):\n",
    "    if Score <=2:\n",
    "        return \"negative\"\n",
    "    elif Score == 3:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"positive\"\n",
    "\n",
    "df['Sentiment'] = df['Score'].apply(map_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ccd25a-8045-4e44-866d-e27d6eb8d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # keep only letters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    words = [w for w in text.split() if w not in stop_words]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94f45a6-c302-40cc-9474-2099863fe5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clean_text'] = df['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f49c85c-f439-4e9c-984f-72fa67d55022",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02bc856-0e80-4366-8770-7985ef49714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Exploration EDA\n",
    "\n",
    "# Sentiment distribution\n",
    "sns.countplot(x='Sentiment', data=df)\n",
    "plt.title(\"Sentiment Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Average text length\n",
    "df['Text_Length'] = df['Clean_text'].apply(len)\n",
    "sns.histplot(df['Text_Length'], bins=50)\n",
    "plt.title(\"Text Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0205c14-b70f-45f6-a438-b2344b0d5f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 454763\n",
      "Test size: 113691\n"
     ]
    }
   ],
   "source": [
    "#Train test split\n",
    "#import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df['Clean_text']\n",
    "y = df['Sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Test size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de54ec4-6ab3-40fd-b095-32f1648386af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87906694461303\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.71      0.74     16407\n",
      "     neutral       0.59      0.24      0.34      8528\n",
      "    positive       0.91      0.97      0.94     88756\n",
      "\n",
      "    accuracy                           0.88    113691\n",
      "   macro avg       0.75      0.64      0.67    113691\n",
      "weighted avg       0.86      0.88      0.86    113691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Baseline Model (TF-IDF + Logistic Regression)\n",
    "#baseline sentiment classifier\n",
    "#TF-IDF Vectorization: Converts text into numerical features.\n",
    "#Logistic Regression: Fast, interpretable baseline for text classification.\n",
    "\n",
    "#import libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "#TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features = 10000, ngram_range=(1,2))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec =  vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "#Logistic Regression model\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train_vec, y_train)\n",
    "\n",
    "#Predictions\n",
    "\n",
    "y_pred = lr_model.predict(X_test_vec)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dbbe24-1fbc-4a42-96ad-1a055f2f8ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f1dbc1-9933-4156-8be2-db1fb0345189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kpk laptops\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53f943e4651464bbb7c358b0b24742f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/454763 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a80714bfaad4be3be62b7837d943130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/113691 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "D:\\Temp\\ipykernel_9220\\3390157578.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\kpk laptops\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='602' max='113692' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   602/113692 45:21 < 142:29:39, 0.22 it/s, Epoch 0.01/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='tensorflow')\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "\n",
    "#Advanced Model (BERT Fine-Tuning)\n",
    "#use Transformers (BERT) for state-of-the-art performance.\n",
    "#import libraries \n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Prepare Hugging Face Dataset\n",
    "train_df = pd.DataFrame({'text': X_train, 'label': y_train.map({'negative':0, 'neutral':1, 'positive':2})})\n",
    "test_df = pd.DataFrame({'text': X_test, 'label': y_test.map({'negative':0, 'neutral':1, 'positive':2})})\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "test_ds = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "train_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "\n",
    "# Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dc4915-9b90-4f0a-921c-bd7dfb543d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62cc382-ae49-4cdf-9af0-cc20174ea995",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import bertopic\n",
    "import sentence_transformers\n",
    "import umap\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Sample subset (BERTopic can be memory intensive)\n",
    "sample_texts = df['Clean_text'].sample(2000, random_state=42).tolist()\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# Define KMeans clustering (replace default hdbscan)\n",
    "kmeans_model = MiniBatchKMeans(n_clusters=20, random_state=42)\n",
    "\n",
    "# Pass the custom clustering model to BERTopic\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    hdbscan_model=kmeans_model,   # replaces hdbscan\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(sample_texts)\n",
    "\n",
    "# Show top topics\n",
    "topic_model.get_topic_info().head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e4ce7f-a49c-4cd5-8254-08b49bb990e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4fdee0-acf0-463b-bdb6-4b028afc12ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart(top_n_topics=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa8c00-9288-4412-b623-b18702d9facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_representative_docs(2)  # for topic 2 (coffee)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae1b71b-8b62-4374-b6fa-9b8f34382590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE THE MODEL\n",
    "\n",
    "import joblib\n",
    "\n",
    "#  Save your Logistic Regression sentiment model and TF-IDF vectorizer\n",
    "joblib.dump(lr_model, \"sentiment_model.pkl\")\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Save your BERTopic model (includes the SentenceTransformer and KMeans)\n",
    "topic_model.save(\"bertopic_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba8f598-a17a-437f-b5a7-b71ad87ee6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e82a73-17c7-41e7-9c80-a3908817ced2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d791454-f99f-4166-a23f-200ff1550dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
